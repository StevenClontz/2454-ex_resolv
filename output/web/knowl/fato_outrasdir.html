<!DOCTYPE html>
<html lang="pt-BR" dir="ltr">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<article class="fact theorem-like"><h3 class="heading">
<span class="type">Fato</span><span class="space"> </span><span class="codenumber">A.6.22</span><span class="period">.</span><span class="space"> </span><span class="title">Outras direções.</span>
</h3>
<div class="para logical">
<div class="para">Podemos generalizar um pouco o resultado acima: se uma função é diferenciável em <span class="process-math">\((x_0,y_0)\text{,}\)</span> basta conhecer as derivadas direcionais nesse ponto, em duas direções linearmente independentes, para determinarmos a derivada direcional em <span class="process-math">\((x_0,y_0)\)</span> em qualquer direção. Para tanto, se <span class="process-math">\(\vec{u}\)</span> e <span class="process-math">\(\vec{v}\)</span> são vetores unitários que definem tais direções, temos que <span class="process-math">\(\big\{\vec{u},\vec{v}\big\}\)</span> é uma base de <span class="process-math">\(\R^2\)</span> e, portanto</div>
<div class="displaymath process-math">
\begin{equation*}
\vec{e_1}=\alpha_1\vec{u}+\beta_1\vec{v}\quad\text{e}\quad
\vec{e_2}=\alpha_2\vec{u}+\beta_2\vec{v}.
\end{equation*}
</div>
</div>
<div class="para logical">
<div class="para">Desta forma, temos</div>
<div class="displaymath process-math">
\begin{equation*}
f_x(x_0,y_0)=\dfrac{\partial
f}{\partial\vec{e_1}}(x_0,y_0)=\big\langle\nabla
f(x_0,y_0),\vec{e_1}\big\rangle=\big\langle\nabla
f(x_0,y_0),\alpha_1\vec{u}+\beta_1\vec{v}\big\rangle.
\end{equation*}
</div>
</div>
<div class="para logical">
<div class="para">Expandindo o produto interno pela sua linearidade e procedento de maneira análoga com o vetor <span class="process-math">\(\vec{e_2}\text{,}\)</span> temos então que</div>
<div class="displaymath process-math">
\begin{align*}
f_x(x_0,y_0)&amp;=\alpha_1\dfrac{\partial f}{\partial
\vec{u}}(x_0,y_0)+\beta_1\dfrac{\partial f}{\partial
\vec{v}}(x_0,y_0);\\
f_y(x_0,y_0)&amp;=\alpha_2\dfrac{\partial f}{\partial
\vec{u}}(x_0,y_0)+\beta_2\dfrac{\partial f}{\partial
\vec{v}}(x_0,y_0).
\end{align*}
</div>
</div>
<div class="para logical">
<div class="para">Mais frequentemente, temos as coordenadas de <span class="process-math">\(\vec{u}\)</span> e <span class="process-math">\(\vec{v}\)</span> na base canônica, digamos <span class="process-math">\(\vec{u}=(a,b)\)</span> e <span class="process-math">\(\vec{v}=(c,d)\text{.}\)</span> Procedendo de maneira inversa, temos um sistema para determinar o gradiente de <span class="process-math">\(f\)</span> (e consequentemente todas as derivadas direcionais):</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{cases}
\dfrac{\partial f}{\partial
\vec{u}}(x_0,y_0)&amp;=af_x(x_0,y_0)+bf_y(x_0,y_0)\\
\dfrac{\partial f}{\partial
\vec{v}}(x_0,y_0)&amp;=cf_x(x_0,y_0)+df_y(x_0,y_0)
\end{cases},
\end{equation*}
</div>
<div class="para">cujas soluções são dadas invertendo-se a matriz <span class="process-math">\(\begin{pmatrix}
a&amp;b\\ c&amp;d \end{pmatrix}\)</span> de coeficientes, o que é possível se e somente se os vetores <span class="process-math">\(\vec{u}\)</span> e <span class="process-math">\(\vec{v}\)</span> são linearmente independentes.</div>
</div></article><span class="incontext"><a class="internal" href="ap_cadeiadir.html#fato_outrasdir">em contexto</a></span>
</body>
</html>
